```{r Library import}
# Feature calculation (+ implicit dependencies)
library(smoof)
library(flacco)
library(lhs)
library(numDeriv)
library(e1071)
library(mda)
library(plyr)
library(RANN)
library(hash)
# Data processing
library(tidyverse)
# Parallel execution
library(foreach)
library(doParallel)
# Model training
library(mlr)
library(parallelMap)
library(rpart)
library(kernlab)
library(randomForest)
library(xgboost)
library(earth)
library(ggallin)
```

# Feature calculation

```{r Feature Calculation}
set.seed(42)

# function for scaling the function values according to prager23
minMaxScaling = function(x){
  return ((x-min(x))/(max(x)-min(x)))
}

# set up parallel execution
cores = detectCores()
cl = makeCluster(cores[1]-1)
registerDoParallel(cl)

# initialize parameters for feature calculation and normalization
feature_sets = c("ela_curv","ela_conv","ela_distr","ela_level","ela_local","ela_meta","basic","cm_angle","disp","ic","nbc","pca")
features_normalization = hash()
features_normalization[["ic"]] = c("ic.eps.s","ic.eps.max","ic.eps.ratio")
features_normalization[["ela_meta"]] = c("ela_meta.lin_simple.intercept","ela_meta.lin_simple.coef.min","ela_meta.lin_simple.coef.max")
features_normalization[["pca"]] = c("pca.expl_var.cov_init","pca.expl_var_PC1.cov_init")
dimensions = c(2L,3L,5L,10L)
fids = 1:24
iids = 1:5

# calculate features for all dimensions X fids X iids 
features= tibble()
# set domain of the sampled functions
control = list("init_sample.lower" = -5, "init_sample.upper" = 5, "init_sample.type" = "lhs")
for (dim in dimensions) {
  X = createInitialSample(n.obs = 50 * dim, dim = dim, control = control)
    # parallel execution per fid
    results_df = foreach(fid=fids, .combine = bind_rows, .packages = c("tidyverse","flacco","smoof")) %dopar% {
      all_features_fid = tibble()
      for (iid in iids) {
          all_features_iid = c()
          for (feature_set in feature_sets){
            # calculate feature set for a function instance
            fn = makeBBOBFunction(dimensions = dim, fid = fid, iid = iid)
            y = apply(X, 1, fn)
            feat.object = createFeatureObject(X = X, y = y, fun = fn, blocks = 3, force = TRUE)
            featureSet = calculateFeatureSet(feat.object, set = feature_set)
            
            # separate calculation for features that need scaled function values as input (according to prager23)
            if(!is.null(features_normalization[[feature_set]])){
              yNorm = minMaxScaling(y)
              feat.objectNorm = createFeatureObject(X = X, y = yNorm, fun = fn, blocks = 3, force = TRUE)
              featureSetNorm = calculateFeatureSet(feat.objectNorm, set = feature_set)
              featureSet[features_normalization[[feature_set]]] = featureSetNorm
            }
            all_features_iid = c(all_features_iid, as_tibble(featureSet))
          }
          row_iid = c(dim = dim, fid = fid, iid = iid, all_features_iid)
          all_features_fid = bind_rows(all_features_fid, row_iid)
      }
      all_features_fid
    }
  features = bind_rows(features, results_df)
  print(paste0("Finished dim ", dim))
}
stopCluster(cl)
```

```{r Feature mean calculation & filtering}
# calculate mean feature values per fid
features = features %>% group_by(dim, fid) %>% summarise_all(mean) %>% select(-iid) %>% ungroup()

# filter out not valid columns (containing INF, NA or constant values)
features[sapply(features, is.infinite)] = NA
features = features[,colSums(is.na(features)) == 0]
features = features[, sapply(features, function(v) var(v, na.rm=TRUE)!=0)]
```

```{r Feature export}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
write.csv(features,"features.csv")
```

# relERT conversion

```{r relERT calculations}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

ert_data = read_csv("ert_data.csv")
# remove repetition column
ert_data = ert_data[-3]
# rename columns so that they are compatible with mlr library
names(ert_data)[names(ert_data) == "CMA-CSA"] = "CMA_CSA"
names(ert_data)[names(ert_data) == "SMAC-BBOB"] = "SMAC_BBOB"

# scale ERT by min ERT for each function
scaleERT = function(x){
  return (x / min(x))
}
df_normalized = t(apply(ert_data[-c(1:2)], 1, scaleERT))
# PAR10 score for INF execution time
PAR10_score = 10 * max(df_normalized[is.finite(df_normalized)])
df_normalized[!is.finite(df_normalized)] = PAR10_score

#  save relERT
relERT = cbind(ert_data[1:2], df_normalized)
write.csv(relERT,"relERT.csv")

# repeat relERT calculation including feature costs
ert_data_fc = cbind(ert_data, t(apply(ert_data, 1, function(row) row[-c(1, 2)] + row[1]*50)))
df_normalized_fc = t(apply(ert_data_fc[-c(1:2)], 1, scaleERT))
df_normalized_fc = df_normalized_fc[,-c(1:12)]
df_normalized_fc[!is.finite(df_normalized_fc)] = PAR10_score

#  save relERT with feature costs
relERT_fc = cbind(ert_data[1:2], as_tibble(df_normalized_fc))
write.csv(relERT_fc,"relERT_featureCosts.csv")
```

# Model training including feature selection

```{r Read data for model training}
# Read pre calculated data
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
features = read_csv("features.csv")
relERT = read_csv("relERT.csv")
relERT_fc = read_csv("relERT_featureCosts.csv")
features = features[-1]
relERT = relERT[-1]
relERT_fc = relERT_fc[-1]
relERT_fc = relERT_fc %>% rename_with(~ gsub(".1", "_FC", .x, fixed = TRUE))
```

```{r Preprocessing}
# remove (almost) constant features, because the cause problems when applying feature selection for ksvm
features = features %>% select(dim, fid, where(~ n_distinct(.) > 6))

# store column names for training
feature_names = colnames(features[3:ncol(features)])
solver_names = colnames(relERT[3:ncol(relERT)])

# scale all features
features[3:ncol(features)] = apply(features[3:ncol(features)], 2, scale)

# calculate best solver per function (random selection for multiple best solvers)
set.seed(42)
relERT$Best = apply(relERT[3:ncol(relERT)], 1, function(x) {
    minima = which(x == min(x))
    colNames = colnames(relERT[3:ncol(relERT)])
    if (length(minima) > 1) {
        return(colNames[sample(minima, 1)])
    } else {
        return(colNames[minima])
    }
})
relERT$Best = as.factor(relERT$Best)

# merge both data sets
data = merge(features, relERT, by=c("dim","fid")) %>% merge(relERT_fc, by=c("dim","fid")) %>% arrange(dim, fid)
```

```{r Feature selection}
strategies = c("all", "sffs", "sfbs", "ga", "ga2")
feature_selection = function(learner, task, strategy){
  set.seed(42)
  if(strategy == "all"){
    return(feature_names)
  }
  rdesc = makeResampleDesc("CV")
  control = switch(
    strategy,
    "sffs" = makeFeatSelControlSequential(method = "sffs"),
    "sfbs" = makeFeatSelControlSequential(method = "sfbs"),
    "ga" = makeFeatSelControlGA(lambda = 5, maxit = 100),
    "ga2" = makeFeatSelControlGA(lambda = 50, maxit = 100)
  )
  cores = detectCores()
  parallelStartSocket(cores[1])
  sf = selectFeatures(learner, task, rdesc, control = control, show.info = FALSE)
  parallelStop()
  return(sf[["x.bit.names"]])
}
```

```{r Classification model validation}
# function for creating the classification learner (inlcuding calculation sigma hyper parameter for ksvm)
createLearnerClassif = function(data, learnerName, selected_features){
  learnerAlgo = makeLearner(learnerName)
  if(learnerName == "classif.ksvm"){
    data_train = data %>% select(all_of(selected_features), "Best")
    sigest_values = sigest(Best~., data = data_train)
    sigma = sigest_values[2] # use mean sigest value
    learnerAlgo = setHyperPars(learnerAlgo, sigma = sigma)
  }
  return(learnerAlgo)
}

# execute model validation for a classification learner
model_validation_classif = function(data, learnerName, selected_features, sigma = NA){
  set.seed(42)
  learnerAlgo = createLearnerClassif(data, learnerName, selected_features)

  # leave-one-(function)-out cross-validation
  relERTPredictions = tibble()
  for(index in 1:nrow(data)){
    # select data for training and validation
    data_train = data[-index, ] %>% select(all_of(selected_features), "Best")
    data_val = data[index,]
    
    # train model
    task = makeClassifTask(data = data_train, target = "Best")
    model = train(learnerAlgo, task)
    
    # prediction for the one function that was left out
    prediction = predict(model, newdata = data_val %>% select(all_of(selected_features)))
    prediction = as.data.frame(prediction)
    predictedAlgo = as.character(prediction[1, 1])
    predictionRelERT = data_val[[predictedAlgo]]
    predictionRelERTFC = data_val[[paste0(predictedAlgo, "_FC")]]
    
    # store prediction
    row = c(dim = data_val$dim, fid = data_val$fid, algo = predictedAlgo, relERT = predictionRelERT, relERT_FC = predictionRelERTFC)
    relERTPredictions = bind_rows(relERTPredictions, row)
  }
  
  relERTPredictions$relERT = as.numeric(relERTPredictions$relERT)
  relERTPredictions$relERT_FC = as.numeric(relERTPredictions$relERT_FC)
  relERTPredictions
}

# evaluate classifiers
learners = c("classif.rpart", "classif.ksvm", "classif.randomForest", "classif.xgboost")
predictions_classif = list()
selectedFeatures_classif = list()

for(learnerName in learners){
  for(strategy in strategies){
    name = paste(learnerName, "_", strategy)
    
    # feature selection
    data_train = data %>% select(all_of(feature_names), "Best")
    learner = createLearnerClassif(data, learnerName, feature_names)
    task = makeClassifTask(data = data_train, target = "Best")
    sel_features = feature_selection(learner, task, strategy)
    if(length(sel_features) == 0){
      next
    }
    selectedFeatures_classif[[name]] = sel_features
    
    # model validation
    predicted_relERT = model_validation_classif(data, learnerName, sel_features)
    predictions_classif[[name]] = predicted_relERT
    
    # print performance data
    print(paste0(learnerName, "_", strategy, " : ", "relERT (ex): ", mean(predicted_relERT$relERT), ", relERT (in): ", mean(predicted_relERT$relERT_FC)))
  }
}

# save collected performance data
saveRDS(predictions_classif, file="predictions_classif.RData")
saveRDS(selectedFeatures_classif, file="selectedFeatures_classif.RData")
```

```{r Regression model validation}
model_validation_regr = function(data, learner, selected_features){
  # leave-one-(function)-out cross-validation
  set.seed(42)
  relERTPredictions = c()
  solver_names = solver_names
  
  # parallel execution of LOO-CV
  cores = detectCores()
  cl = makeCluster(cores[1]-1)
  registerDoParallel(cl)
  relERTPredictions = foreach(index=1:nrow(data), .combine = bind_rows, .packages = c("tidyverse", "mlr")) %dopar% {
    # select data for training and validation
    data_val = data[index,]
    solverRelERTPred = structure(numeric(0), names=character(0))
    for(solver in solver_names){
        data_train = data[-index, ] %>% select(all_of(selected_features), all_of(solver))
        
        # train model
        learnerAlgo = makeLearner(learner)
        task = makeRegrTask(data = data_train, target = solver)
        model = train(learnerAlgo, task)
        
        # prediction for the one function that was left out
        prediction = predict(model, newdata = data_val %>% select(all_of(selected_features)))
        prediction = as.data.frame(prediction)
        predictedRelERT = prediction[1, 1]
        solverRelERTPred[solver] = predictedRelERT
    }

    # choose best algorithm for the left-out function based on the predicted relERT values
    predictedAlgo = names(solverRelERTPred[solverRelERTPred == min(solverRelERTPred)])[1]
    predictionRelERT = data_val[[predictedAlgo]]
    predictionRelERTFC = data_val[[paste0(predictedAlgo, "_FC")]]
    
     # store prediction
    row = c(dim = data_val$dim, fid = data_val$fid, algo = predictedAlgo, relERT = predictionRelERT, relERT_FC = predictionRelERTFC)
    row
  }
  
  stopCluster(cl)
  relERTPredictions$relERT = as.numeric(relERTPredictions$relERT)
  relERTPredictions$relERT_FC = as.numeric(relERTPredictions$relERT_FC)
  relERTPredictions
}

# evaluate regression models
learners = c("regr.rpart", "regr.ksvm", "regr.randomForest", "regr.xgboost", "regr.earth")
predictions_regr = list()
selectedFeatures_regr = list()

for(learnerName in learners){
  for(strategy in strategies){
    name = paste(learnerName, "_", strategy)
    
    # feature selection
    data_train = data %>% select(all_of(feature_names), "BSqi")
    learner = makeLearner(learnerName)
    task = makeRegrTask(data = data_train, target = "BSqi")
    sel_features = feature_selection(learner, task, strategy)
    if(length(sel_features) == 0){
      next
    }
    selectedFeatures_regr[[name]] = sel_features
    
    # model validation
    predicted_relERT = model_validation_regr(data, learnerName, sel_features)
    predictions_regr[[name]] = predicted_relERT
    
    # print performance data
    print(paste0(learnerName, "_", strategy, " : ", "relERT (ex): ", mean(predicted_relERT$relERT), ", relERT (in): ", mean(predicted_relERT$relERT_FC)))
  }
}

# save collected performance data
saveRDS(predictions_regr, file="predictions_regr.RData")
saveRDS(selectedFeatures_regr, file="selectedFeatures_regr.RData")
```

```{r Train paired regression model}
# configure training
validationFunctionIndex = 1
learnerName = "regr.randomForest"
strategy = "all"

# feature selection
data_train = data %>% 
          select(all_of(feature_names), "BSqi", "BSrr") %>%
          mutate(solverDiff = .data$BSqi - .data$BSrr) %>%
          select(-"BSqi", -"BSrr")
learner = makeLearner(learnerName)
task = makeRegrTask(data = data_train, target = "solverDiff")
selected_features = feature_selection(learner, task, strategy)

 # select validation data
data_val = data[validationFunctionIndex,]

# initialize matrix for the predicted relative differences of the solver pairs 
solverDiffs = matrix(0, length(solver_names), length(solver_names))
rownames(solverDiffs) = solver_names
colnames(solverDiffs) = solver_names

# train model for each solver pair
set.seed(42)
for(solverA in 1:(length(solver_names)-1)){
  for(solverB in (solverA+1):length(solver_names)){
    # select training data
    solverAName = solver_names[solverA]
    solverBName = solver_names[solverB]
    data_train = data[-validationFunctionIndex, ] %>% 
      select(all_of(selected_features), all_of(solverAName), all_of(solverBName)) %>%
      mutate(solverDiff := .data[[solverAName]] - .data[[solverBName]]) %>%
      select(-all_of(solverAName), -all_of(solverBName))
    
    # train model
    learnerAlgo = makeLearner(learnerName)
    task = makeRegrTask(data = data_train, target = "solverDiff")
    model = train(learnerAlgo, task)
    
    # prediction for the one function that was left out
    prediction = predict(model, newdata = data_val %>% select(all_of(selected_features)))
    prediction = as.data.frame(prediction)
    predictedDiff = prediction[1, 1]
    solverDiffs[solverA, solverB] = predictedDiff
    solverDiffs[solverB, solverA] = -predictedDiff
  }
}

# choose best algorithm for the left-out function based on the predicted relERT differences
sumDiffs = rowSums(solverDiffs)
solverDiffs = cbind(solverDiffs, sumDiffs)
predictedAlgo = names(sumDiffs[sumDiffs == min(sumDiffs)])[1]
predictedAlgoRelERT = data_val[[predictedAlgo]]
predictedAlgoRelERTFC = data_val[[paste0(predictedAlgo, "_FC")]]
```

```{r Paired regression model validation}
model_validation_pair_regr = function(data, learner, selected_features){
  # leave-one-(function)-out cross-validation
  set.seed(42)
  relERTPredictions = c()
  solver_names = solver_names
  
  # parallel execution of LOO-CV
  cores = detectCores()
  cl = makeCluster(cores[1]-1)
  registerDoParallel(cl)
  relERTPredictions = foreach(index=1:nrow(data), .combine = bind_rows, .packages = c("tidyverse", "mlr")) %dopar% {
    # select validation data
    data_val = data[index,]
    
    # initialize matrix for the predicted relative differences of the solver pairs 
    solverDiffs = matrix(0, length(solver_names), length(solver_names))
    rownames(solverDiffs) = solver_names
    colnames(solverDiffs) = solver_names
    
    # train model for each solver pair
    for(solverA in 1:(length(solver_names)-1)){
      for(solverB in (solverA+1):length(solver_names)){
        # select training data
        solverAName = solver_names[solverA]
        solverBName = solver_names[solverB]
        data_train = data[-index, ] %>% 
          select(all_of(selected_features), all_of(solverAName), all_of(solverBName)) %>%
          mutate(solverDiff := .data[[solverAName]] - .data[[solverBName]]) %>%
          select(-all_of(solverAName), -all_of(solverBName))
        
        # train model
        learnerAlgo = makeLearner(learner)
        task = makeRegrTask(data = data_train, target = "solverDiff")
        model = train(learnerAlgo, task)
        
        # prediction for the one function that was left out
        prediction = predict(model, newdata = data_val %>% select(all_of(selected_features)))
        prediction = as.data.frame(prediction)
        predictedDiff = prediction[1, 1]
        solverDiffs[solverA, solverB] = predictedDiff
        solverDiffs[solverB, solverA] = -predictedDiff
      }
    }
    
    # choose best algorithm for the left-out function based on the predicted relERT differences
    sumDiffs = rowSums(solverDiffs)
    predictedAlgo = names(sumDiffs[sumDiffs == min(sumDiffs)])[1]
    predictionRelERT = data_val[[predictedAlgo]]
    predictionRelERTFC = data_val[[paste0(predictedAlgo, "_FC")]]
    
    # store prediction
    row = c(dim = data_val$dim, fid = data_val$fid, algo = predictedAlgo, relERT = predictionRelERT, relERT_FC = predictionRelERTFC)
    row
  }
  
  stopCluster(cl)
  relERTPredictions$relERT = as.numeric(relERTPredictions$relERT)
  relERTPredictions$relERT_FC = as.numeric(relERTPredictions$relERT_FC)
  relERTPredictions
}

# evaluate pairwise regression models
learners = c("regr.rpart", "regr.ksvm", "regr.randomForest", "regr.xgboost", "regr.earth")
predictions_pair_regr = list()
selectedFeatures_pair_regr = list()
for(learnerName in learners){
  for(strategy in strategies){
    name = paste(learnerName, "_", strategy)
    
    # feature selection
    data_train = data %>% 
          select(all_of(feature_names), "BSqi", "BSrr") %>%
          mutate(solverDiff = .data$BSqi - .data$BSrr) %>%
          select(-"BSqi", -"BSrr")
    learner = makeLearner(learnerName)
    task = makeRegrTask(data = data_train, target = "solverDiff")
    sel_features = feature_selection(learner, task, strategy)
    if(length(sel_features) == 0){
      next
    }
    selectedFeatures_pair_regr[[name]] = sel_features
    
    # model validation
    predicted_relERT = model_validation_pair_regr(data, learnerName, sel_features)
    predictions_pair_regr[[name]] = predicted_relERT
    
    # print performance data
    print(paste0(learnerName, "_", strategy, " : ", "relERT (ex): ", mean(predicted_relERT$relERT), ", relERT (in): ", mean(predicted_relERT$relERT_FC)))
  }
}

# save collected performance data
saveRDS(predictions_pair_regr, file="predictions_pair_regr.RData")
saveRDS(selectedFeatures_pair_regr, file="selectedFeatures_pair_regr.RData")
```

# Aggregate & plot results

```{r Plot model results}
# load relERT data
relERT = read_csv("relERT.csv")[-1]

# calculate SBS
solver_names = colnames(relERT[3:ncol(relERT)])
meanRelERT = colMeans(relERT[solver_names])
SBSrelERT = min(meanRelERT)
SBS = names(meanRelERT)[meanRelERT == SBSrelERT]
SBSData = relERT %>% select(dim, fid, all_of(SBS))

# create plots for relERT of a model vs. SBS
relErtPlot = function(model, type, title){
  # initialize data frame for plotting
  df = merge(model, SBSData, by=c("dim","fid"))
  df$dim = factor(df$dim, levels = c(2,3,5,10))
  df$fid = factor(df$fid, levels = 1:24)
  df = df %>% arrange(dim, fid)
  df = df %>% mutate(diff = relERT_FC-HCMA)
  
  # plot relERT vs. SBS relERT
  p1 = ggplot(df, aes(x = relERT, y = HCMA, color = fid)) + 
    geom_point() + 
    scale_x_log10() + 
    scale_y_log10() + 
    geom_abline(slope = 1, intercept = 0, color = "grey80", linetype = "dashed") + 
    theme_bw() + 
    theme(legend.position = "bottom")  +
    ggtitle(title) +
    xlab(paste("relERT of the best", title, "solver")) +
    ylab("relERT of HCMA solver") +
    labs(color= "Function ID")

  # plot relERT including feature costs vs. SBS relERT
  p2 = ggplot(df, aes(x = relERT_FC, y = HCMA, color = fid)) + 
    geom_point() + 
    scale_x_log10() + 
    scale_y_log10() + 
    geom_abline(slope = 1, intercept = 0, color = "grey80", linetype = "dashed") + 
    theme_bw() + 
    theme(legend.position = "bottom") +
    ggtitle(title) +
    xlab("relERT including feature costs") +
    ylab("relERT of HCMA solver") +
    labs(color= "Function ID")
  
  # plot diference between relERT including feature costs and SBS relERT
  p3 = ggplot(df, aes(x=fid, y=diff, fill = dim)) +
    geom_col(position = "dodge") +
    scale_y_continuous(trans = pseudolog10_trans, breaks = c(-1000, -100, -10, 0, 10, 100, 1000, 10000)) +
    ggtitle(title) +
    xlab("Function ID") +
    ylab(expression(Delta ~ "relERT")) +
    labs(fill= "Dimensions")
  
  # plot relERT including feature costs of model
  p4 = ggplot(df, aes(x = fid, y = relERT_FC, fill = dim)) + 
    geom_col(position = "dodge") + 
    scale_y_continuous(trans='log10') +
    theme(legend.position = "bottom") +
    ggtitle(title) +
    xlab("Function ID") +
    ylab("relERT including feature costs") +
    labs(fill= "Dimensions")
  
  # display plots
  print(p1)
  print(p2)
  print(p3)
  print(p4)
}

# evaluate performance data for each model type
files = c("predictions_classif.RData","predictions_regr.RData", "predictions_pair_regr.RData")
titles = c("Classification", "Regression", "Pairwise Regression")
for (file in 1:3){
  predictions = readRDS(files[file])

  # print relERTs of every model
  #for (name in names(predictions)){
  #  model = predictions[[name]]
  #  print(paste0("relERT model ", name ,": ", mean(model$relERT)))
  #  print(paste0("relERT model ", name, " with FC:", mean(model$relERT_FC)))
  #}
  
  # determine best model
  bestModelName = names(predictions)[which.min(sapply(predictions, function(x) mean(x$relERT)))]
  bestModel = predictions[[bestModelName]]
  print(paste0("best model: ", bestModelName))
  print(paste0("best model relERT: ", mean(bestModel$relERT)))
  print(paste0("best model relERT with FC: ", mean(bestModel$relERT_FC)))
  
  # create plots for best model
  relErtPlot(bestModel, files[file], titles[file])
}
```

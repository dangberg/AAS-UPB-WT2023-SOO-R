---
title: "R Notebook"
---

```{r Library import}
# Feature calculation (+ implicit dependencies)
library(smoof)
library(flacco)
library(lhs)
library(numDeriv)
library(e1071)
library(mda)
library(plyr)
library(RANN)
# Data processing
library(tidyverse)
# Parallel execution
library(foreach)
library(doParallel)
# Model training
library(mlr)
library(rpart)
library(kernlab)
library(randomForest)
library(xgboost)
library(earth)
```

```{r Feature Calculation}
set.seed(42)

cores = detectCores()
cl = makeCluster(cores[1]-1)
registerDoParallel(cl)

feature_sets <- c("ela_curv","ela_conv","ela_distr","ela_level","ela_local","ela_meta","basic","cm_angle","disp","ic","nbc","pca")
dimensions = c(2L,3L,5L,10L)
fids = 1:24
iids = 1:5


features= tibble()
#Set domain of the sampled functions
control = list("init_sample.lower" = -5, "init_sample.upper" = 5, "init_sample.type" = "lhs")
for (dim in dimensions) {
  X <- createInitialSample(n.obs = 50 * dim, dim = dim, control = control)
  
    results_df = foreach(fid=fids, .combine = bind_rows, .packages = c("tidyverse","flacco","smoof")) %dopar% {
      blub = tibble()
      for (iid in iids) {
          all_features = c()
          for (feature_set in feature_sets){
            fn = makeBBOBFunction(dimensions = dim, fid = fid, iid = iid)
            feat.object = createFeatureObject(X = X, fun = fn)
            featureSet = calculateFeatureSet(feat.object, set = feature_set)
            all_features = c(all_features, as_tibble(featureSet))
          }
          sample_row = c(dim = dim, fid = fid, iid = iid, all_features)
          blub = bind_rows(blub,sample_row)
      }
      blub
    }
  features = bind_rows(features,results_df)
  print(paste0("Finished dim ", dim))
}
stopCluster(cl)
```

```{r Feature Scaling}
normalize = function(x){
  return ((x-min(x))/(max(x)-min(x)))
}

normalized_features = features %>% mutate_at(c("ic.eps.s","ic.eps.max","ic.eps.ratio","ela_meta.lin_simple.intercept","ela_meta.lin_simple.coef.min","ela_meta.lin_simple.coef.max","pca.expl_var.cov_init","pca.expl_var_PC1.cov_init"), normalize)

normalized_features = normalized_features %>% group_by(dim, fid) %>% summarise_all(mean) %>% select(-iid) %>% ungroup()
```

```{r Export}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
write.csv(normalized_features,"features.csv")
```

```{r relERT calculation}
scale = function(x){
  return (x / min(x))
}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
ert_data = read_csv("ert_data.csv")
ert_data = ert_data[-3]

df_normalized = t(apply(ert_data[-c(1:2)], 1, scale))
PAR10_score = 10 * max(df_normalized[is.finite(df_normalized)])
df_normalized[!is.finite(df_normalized)] = PAR10_score

relERT = cbind(ert_data[1:2], df_normalized)
write.csv(relERT,"relERT.csv")

#relERT calculation with feature costs
ert_data_fc = cbind(ert_data, t(apply(ert_data, 1, function(row) row[-c(1, 2)] + row[1]*50)))

df_normalized_fc = t(apply(ert_data_fc[-c(1:2)], 1, scale))

df_normalized_fc = df_normalized_fc[,-c(1:12)]

df_normalized_fc[!is.finite(df_normalized_fc)] = PAR10_score

relERT_fc = cbind(ert_data[1:2], as.tibble(df_normalized_fc))
write.csv(relERT_fc,"relERT_featureCosts.csv")
```

```{r Read data}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
features = read_csv("features.csv")
relERT = read_csv("relERT.csv")
features = features[-1]
relERT = relERT[-1]
```

```{r Preprocessing}
features[sapply(features, is.infinite)] = NA
features <- features[,colSums(is.na(features)) == 0]
features <- features[, sapply(features, function(v) var(v, na.rm=TRUE)!=0)]


names(relERT)[names(relERT) == "CMA-CSA"] <- "CMA_CSA"
names(relERT)[names(relERT) == "SMAC-BBOB"] <- "SMAC_BBOB"
feature_names = colnames(features[3:ncol(features)])
solver_names = colnames(relERT[3:ncol(relERT)])

set.seed(42)
relERT$Best <- apply(relERT[3:ncol(relERT)], 1, function(x) {
    minima <- which(x == min(x))
    colNames <- colnames(relERT[3:ncol(relERT)])
    if (length(minima) > 1) {
        return(colNames[sample(minima, 1)])
    } else {
        return(colNames[minima])
    }
})


relERT$Best = as.factor(relERT$Best)

data = merge(features,relERT,by=c("dim","fid"))
```

```{r regression model training}
learners = c("regr.rpart", "regr.ksvm", "regr.randomForest", "regr.xgboost", "regr.earth")
regr_models = list()
for(learner in learners)
{
  regr.lrn = makeLearner(learner)
  for(solver in solver_names){
    name = paste(learner, solver)
    data_temp = data %>% select(all_of(feature_names), solver)
    regr.task = makeRegrTask(id = name, data = data_temp, target = solver)
    model = train(regr.lrn, regr.task)
    regr_models[[name]] = model
  }
}
```

```{r classification model training}
data_temp = data %>% select(all_of(feature_names), "Best")
sigest_values = sigest(Best~., data = data_temp)
sigma = sigest_values[2] # use mean sigest value
learners = c("classif.rpart", "classif.ksvm", "classif.randomForest", "classif.xgboost")
classif_models = list()
for (learner in learners){
  classif.lrn = makeLearner(learner)
  if(learner == "classif.ksvm"){
    classif.lrn = setHyperPars(classif.lrn, sigma = sigma)
  }
  classif.task = makeClassifTask(id = learner, data = data_temp, target = "Best")
  model = train(classif.lrn, classif.task)
  classif_models[[learner]] = model
}
```

```{r paired regression model training}
learners = c("regr.rpart", "regr.ksvm", "regr.randomForest", "regr.xgboost", "regr.earth")
# TODO: implement
```



